{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Developing LLM Applications with LangChain\n",
                "\n",
                "## Introduction to LangChain & Chatbot Mechanics\n",
                "\n",
                "### Hugging Face models in LangChain!\n",
                "\n",
                "There are thousands of language models freely available to use on\n",
                "[Hugging Face](https://huggingface.co/models). Hugging Face integrates\n",
                "really nicely into LangChain, so in this exercise, you'll use LangChain\n",
                "to load and predict using a model from Hugging Face.\n",
                "\n",
                "To complete this exercise, you'll need first need to create a Hugging\n",
                "Face API token. Creating this token is completely free, and there are no\n",
                "charges for loading models.\n",
                "\n",
                "1.  Sign up for a Hugging Face account at `https://huggingface.co/join`\n",
                "2.  Navigate to `https://huggingface.co/settings/tokens`\n",
                "3.  Select \"New token\" and copy the key\n",
                "\n",
                "![The Hugging Face webpage for creating new\n",
                "tokens.](https://assets.datacamp.com/production/repositories/6487/datasets/f43d687d38db657069c547855f5a79bb12a28fbf/hf-signup.png)\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your Hugging Face API key to `huggingfacehub_api_token`.\n",
                "- Define an LLM using the Falcon-7B instruct model from Hugging Face,\n",
                "  which has the ID: `'tiiuae/falcon-7b-instruct'`.\n",
                "- Use `llm` to predict the next words after the text in `question`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "from dotenv import load_dotenv\n",
                "import os\n",
                "\n",
                "load_dotenv()\n",
                "huggingfacehub_api_token = os.environ[\"HUGGINGFACE_API_KEY\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
                        "  warn_deprecated(\n",
                        "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Whatever you do, take care of your shoes.\n",
                        "I'm a big fan of the \"take care of your shoes\" philosophy. I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
                        "I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
                        "I've had a lot of shoes in my life, and I\n"
                    ]
                }
            ],
            "source": [
                "from langchain_community.llms import HuggingFaceHub\n",
                "\n",
                "# Set your Hugging Face API token \n",
                "# huggingfacehub_api_token = '<HUGGING_FACE_TOKEN>'  # added/edited\n",
                "\n",
                "# Define the LLM\n",
                "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
                "\n",
                "# Predict the words following the text in question\n",
                "question = 'Whatever you do, take care of your shoes'\n",
                "output = llm.invoke(question)\n",
                "\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### OpenAI models in LangChain!\n",
                "\n",
                "OpenAI's models are particularly well-regarded in the AI/LLM community;\n",
                "their high performance is largely part to the use of their proprietary\n",
                "technology and carefully selected training data. In contrast to the\n",
                "open-source models on Hugging Face, OpenAI's models do have costs\n",
                "associated with their use, but for many applications, they are currently\n",
                "the best choice to build on.\n",
                "\n",
                "Due to LangChain's unified syntax, swapping one model for another only\n",
                "requires changing a small amount of code. In this exercise, you'll do\n",
                "just that!\n",
                "\n",
                "To use OpenAI's models, you'll need an OpenAI API key. If you haven't\n",
                "created one of these before, first, visit their [signup\n",
                "page](https://platform.openai.com/signup). Next, navigate to the [API\n",
                "keys page](https://platform.openai.com/account/api-keys) to create your\n",
                "secret key. If you've lost your key, you can create a new one here, too.\n",
                "\n",
                "<img\n",
                "src=\"https://assets.datacamp.com/production/repositories/6309/datasets/842da12a5b68c9f3240978dcfb08726b57ee2a18/api-key-page.png\"\n",
                "style=\"width:100.0%\" alt=\"The button to create a new secret key.\" />\n",
                "\n",
                "OpenAI sometimes provides free credits to new users of the API, but this\n",
                "can differ depending on geography. You may also need to add debit/credit\n",
                "card details depending on geography and available credit. **You'll need\n",
                "less than \\$1 credit to complete this course.**\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define an LLM using the default OpenAI model available on LangChain.\n",
                "- Use the OpenAI `llm` to predict the next words after the text in\n",
                "  `question`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import openai\n",
                "\n",
                "load_dotenv()\n",
                "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        ".\n",
                        "\n",
                        "The right pair of shoes can make all the difference in your daily comfort and overall foot health. And when you invest in a quality pair of shoes, you want to make sure they last as long as possible. Proper shoe maintenance is essential for extending the life of your shoes and keeping them in good condition.\n",
                        "\n",
                        "Here are five tips for taking care of your shoes:\n",
                        "\n",
                        "1. Clean and protect your shoes regularly.\n",
                        "\n",
                        "Dirt and debris can cause damage to your shoes over time. It's important to clean your shoes regularly, depending on the material. For leather shoes, use a leather cleaner or conditioner to remove dirt and moisturize the leather. For canvas shoes, a mixture of water and mild detergent can be used to scrub away dirt. After cleaning, make sure to protect your shoes with a waterproof spray or polish to prevent water and stains from damaging the material.\n",
                        "\n",
                        "2. Rotate your shoes.\n",
                        "\n",
                        "Wearing the same pair of shoes every day can lead to wear and tear, and can also cause foot problems such as blisters. It's important to rotate your shoes and give them a break. This allows them to air out and regain their shape. Ideally, you should have at least two pairs of shoes that you can alternate between.\n",
                        "\n",
                        "3. Store your shoes properly.\n",
                        "\n",
                        "Proper storage\n"
                    ]
                }
            ],
            "source": [
                "from langchain_openai import OpenAI\n",
                "\n",
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = \"<OPENAI_API_TOKEN>\"  # added/edited\n",
                "\n",
                "# Define the LLM\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
                "\n",
                "# Predict the words following the text in question\n",
                "question = 'Whatever you do, take care of your shoes'\n",
                "output = llm.invoke(question)\n",
                "\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt templates and chaining\n",
                "\n",
                "In this exercise, you'll begin using two of the core components in\n",
                "LangChain: prompt templates and chains!\n",
                "\n",
                "**Prompt templates** are used for creating prompts in a more modular\n",
                "way, so they can be reused and built on. **Chains** act as the glue in\n",
                "LangChain; bringing the other components together into workflows that\n",
                "pass inputs and outputs between the different components.\n",
                "\n",
                "The classes necessary for completing this exercise, including\n",
                "`HuggingFaceHub`, have been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your Hugging Face API key to `huggingfacehub_api_token`.\n",
                "- Convert the `template` text provided into a LangChain prompt template.\n",
                "- Create an LLM chain to integrate the prompt template and LLM.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: langchain in /home/vscode/.local/lib/python3.12/site-packages (0.1.17)\n",
                        "Requirement already satisfied: PyYAML>=5.3 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
                        "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (2.0.29)\n",
                        "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
                        "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (0.6.5)\n",
                        "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (1.33)\n",
                        "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (0.0.36)\n",
                        "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (0.1.49)\n",
                        "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (0.0.1)\n",
                        "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (0.1.53)\n",
                        "Requirement already satisfied: numpy<2,>=1 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
                        "Requirement already satisfied: pydantic<3,>=1 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (2.7.1)\n",
                        "Requirement already satisfied: requests<3,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
                        "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
                        "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vscode/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
                        "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
                        "Requirement already satisfied: jsonpointer>=1.9 in /home/vscode/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
                        "Requirement already satisfied: packaging<24.0,>=23.2 in /home/vscode/.local/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
                        "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/vscode/.local/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.2)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
                        "Requirement already satisfied: pydantic-core==2.18.2 in /home/vscode/.local/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.6.1 in /home/vscode/.local/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
                        "Requirement already satisfied: greenlet!=0.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
                        "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install langchain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "from langchain_core.prompts.prompt import PromptTemplate\n",
                "from langchain.chains.llm import LLMChain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "You are an artificial intelligence assistant, answer the question. How does LangChain make LLM application development easier?\n",
                        "LangChain provides a platform for LLM application development that simplifies the process by offering a range of tools and services. It allows developers to focus on the core of their application while handling the low-level details such as database management, authentication, and authorization. LangChain also offers a variety of pre-built modules and components that can be easily integrated into LLM applications, reducing development time and cost. Additionally, LangChain provides a secure and scalable infrastructure that can handle large volumes of traffic and data\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
                        "  warn_deprecated(\n",
                        "/home/vscode/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
                        "  warn_deprecated(\n"
                    ]
                }
            ],
            "source": [
                "# Set your Hugging Face API token\n",
                "# huggingfacehub_api_token = '<HUGGING_FACE_TOKEN>'  # added/edited\n",
                "\n",
                "# Create a prompt template from the template string\n",
                "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
                "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
                "\n",
                "# Create a chain to integrate the prompt template and LLM\n",
                "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
                "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
                "\n",
                "question = \"How does LangChain make LLM application development easier?\"\n",
                "print(llm_chain.run(question))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Chat prompt templates\n",
                "\n",
                "Given the importance of chat models in many different LLM applications,\n",
                "LangChain provides functionality for accessing chat-specific models and\n",
                "chat prompt templates.\n",
                "\n",
                "In this exercise, you'll define a chat model from OpenAI, and create a\n",
                "prompt template for it to begin sending it user input questions.\n",
                "\n",
                "All of the LangChain classes necessary for completing this exercise have\n",
                "been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define an LLM using an OpenAI chat model.\n",
                "- Convert the messages stored in a list of tuples into a *chat prompt\n",
                "  template*.\n",
                "- Insert the question provided into the template and call the model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "from langchain_core.prompts.chat import ChatPromptTemplate\n",
                "from langchain_openai.chat_models.base import ChatOpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
                        "  warn_deprecated(\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "AIMessage(content=\"To retain learning effectively, you can try the following strategies:\\n\\n1. Review and practice regularly: Consistent review and practice of the material you have learned can help reinforce your memory.\\n\\n2. Teach others: Explaining concepts to someone else can help solidify your understanding and retention of the information.\\n\\n3. Use mnemonic devices: Mnemonics, such as acronyms or visual aids, can help you remember key information more easily.\\n\\n4. Stay engaged: Actively engaging with the material, such as asking questions, taking notes, or participating in discussions, can enhance your retention.\\n\\n5. Apply what you've learned: Applying the knowledge in real-life situations or through practice exercises can help you retain the information better.\\n\\n6. Get enough rest: Adequate sleep is crucial for memory consolidation, so make sure you are well-rested to retain what you have learned.\\n\\nBy incorporating these strategies into your learning routine, you can improve your retention and recall of information.\", response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 27, 'total_tokens': 220}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-1dacc46a-b3f0-4b1a-b386-cbbb61496cd7-0')"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key= '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Define an OpenAI chat model\n",
                "llm = ChatOpenAI(temperature=0)\t\t\n",
                "\n",
                "# Create a chat prompt template\n",
                "prompt_template = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", \"You are a helpful assistant.\"),\n",
                "        (\"human\", \"Respond to question: {question}\")\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Insert a question into the template and call the model\n",
                "full_prompt = prompt_template.format_messages(question='How can I retain learning?')\n",
                "llm(full_prompt)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Integrating a chatbot message history\n",
                "\n",
                "A key feature of chatbot applications is the ability to have a\n",
                "conversation, where context from the conversation is stored and\n",
                "available for the model to access.\n",
                "\n",
                "In this exercise, you'll create a conversation history that will be\n",
                "passed to the model. This history will contain every message in the\n",
                "conversation, including the user inputs and model responses.\n",
                "\n",
                "All of the LangChain classes necessary for completing this exercise have\n",
                "been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Create a conversation history and add the first AI message.\n",
                "- Add the user message and call the model on the messages in the\n",
                "  conversation history.\n",
                "- Add another user message and call the model on the updated message\n",
                "  history.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "content=\"A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable (such as a list, tuple, or range) and filtering the items based on a condition.\\n\\nHere's a simple example of a list comprehension that generates a list of squared numbers from 0 to 9:\\n\\n```python\\nsquared_numbers = [x**2 for x in range(10)]\\nprint(squared_numbers)\\n```\\n\\nThis will output:\\n\\n```\\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\\n```\\n\\nList comprehensions are a powerful and concise way to work with lists in Python and are often preferred over traditional for loops for their readability and simplicity.\" response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 26, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None} id='run-c87b51f8-8d07-4e41-9cba-f325910f753f-0'\n",
                        "content='List comprehension is a concise way to create lists in Python using a single line of code.' response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 36, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None} id='run-92859f38-6dce-4ede-b300-965cc60df4e6-0'\n"
                    ]
                }
            ],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key= '<OPENAI_API_TOKEN>'  # added/edited\n",
                "chat = ChatOpenAI(temperature=0)\n",
                "\n",
                "# Create the conversation history and add the first AI message\n",
                "history = ChatMessageHistory()\n",
                "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
                "\n",
                "# Add the user message to the history and call the model\n",
                "history.add_user_message(\"What is a list comprehension?\")\n",
                "ai_response = chat(history.messages)\n",
                "print(ai_response)\n",
                "\n",
                "# Add another user message and call the model\n",
                "history.add_user_message(\"Describe the same in fewer words\")\n",
                "ai_response = chat(history.messages)\n",
                "print(ai_response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating a memory buffer\n",
                "\n",
                "For many applications, storing and accessing the entire conversation\n",
                "history isn't technically feasible. In these cases, the messages must be\n",
                "condensed while retaining as much relevant context as possible. One\n",
                "common way of doing this is with a memory buffer, which stores only the\n",
                "most recent messages.\n",
                "\n",
                "In this exercise, you'll integrate a memory buffer into an OpenAI chat\n",
                "model using a chain.\n",
                "\n",
                "All of the LangChain classes necessary for completing this exercise have\n",
                "been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a buffer memory that stores the *four* most recent messages.\n",
                "- Define a conversation chain for integrating the model and memory\n",
                "  buffer; set `verbose` to `True`.\n",
                "- Invoke the chain with the inputs provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.memory.buffer import ConversationBufferMemory\n",
                "from langchain.chains.conversation.base import ConversationChain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
                        "Prompt after formatting:\n",
                        "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
                        "\n",
                        "Current conversation:\n",
                        "\n",
                        "Human: Write Python code to draw a scatter plot.\n",
                        "AI:\u001b[0m\n",
                        "\n",
                        "\u001b[1m> Finished chain.\u001b[0m\n",
                        "\n",
                        "\n",
                        "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
                        "Prompt after formatting:\n",
                        "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
                        "\n",
                        "Current conversation:\n",
                        "Human: Write Python code to draw a scatter plot.\n",
                        "AI:  Sure, I can definitely help you with that! To draw a scatter plot in Python, you will need to import the matplotlib library. This can be done by using the command \"import matplotlib.pyplot as plt\". Then, you can create a scatter plot by using the \"scatter()\" function and passing in the x and y values as parameters. For example, if you have two lists of data called \"x_data\" and \"y_data\", you can create a scatter plot by using the code \"plt.scatter(x_data, y_data)\". You can also customize your scatter plot by adding labels, titles, and changing the color and size of the points. Is there anything else you would like to know about drawing a scatter plot in Python?\n",
                        "Human: Use the Seaborn library.\n",
                        "AI:\u001b[0m\n",
                        "\n",
                        "\u001b[1m> Finished chain.\u001b[0m\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "' Ah, great question! Seaborn is another popular library for data visualization in Python. To use Seaborn for a scatter plot, you will need to import it using the command \"import seaborn as sns\". Then, you can create a scatter plot by using the \"scatterplot()\" function and passing in the x and y values as well as the data parameter. For example, if you have a dataframe called \"df\" with columns \"x\" and \"y\", you can create a scatter plot by using the code \"sns.scatterplot(x=\\'x\\', y=\\'y\\', data=df)\". Seaborn also offers many customization options for scatter plots, such as adding a regression line or changing the shape and color of the points. Is there anything else you would like to know about using Seaborn for scatter plots?'"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
                "\n",
                "# Define a buffer memory\n",
                "memory = ConversationBufferMemory(size=4)\n",
                "\n",
                "# Define the chain for integrating the memory with the model\n",
                "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
                "\n",
                "# Invoke the chain with the inputs provided\n",
                "buffer_chain.predict(input=\"Write Python code to draw a scatter plot.\")\n",
                "buffer_chain.predict(input=\"Use the Seaborn library.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Implementing a summary memory\n",
                "\n",
                "For longer conversations, storing the entire memory, or even a long\n",
                "buffer memory, may not be technically feasible. In these cases, a\n",
                "summary memory implementation can be a good option. Summary memories\n",
                "summarize the conversation at each step to retain the key context for\n",
                "the model to use. This works by using another LLM for generating the\n",
                "summaries, alongside the LLM used for generating the responses.\n",
                "\n",
                "In this exercise, you'll implement a chatbot summary memory, using an\n",
                "OpenAI model for generating the summaries.\n",
                "\n",
                "All of the LangChain classes necessary for completing this exercise have\n",
                "been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a summary memory that uses the same OpenAI model for generating\n",
                "  the summaries.\n",
                "- Define a conversation chain for integrating the model and summary\n",
                "  memory; set `verbose` to `True`.\n",
                "- Invoke the chain with the inputs provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'ConversationSummaryMemory' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m chat \u001b[38;5;241m=\u001b[39m OpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define a summary memory that uses an OpenAI model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43mConversationSummaryMemory\u001b[49m(llm\u001b[38;5;241m=\u001b[39mOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define the chain for integrating the memory with the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m summary_chain \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39mchat, memory\u001b[38;5;241m=\u001b[39mmemory, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'ConversationSummaryMemory' is not defined"
                    ]
                }
            ],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
                "\n",
                "# Define a summary memory that uses an OpenAI model\n",
                "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\"))\n",
                "\n",
                "# Define the chain for integrating the memory with the model\n",
                "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
                "\n",
                "# Invoke the chain with the inputs provided\n",
                "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
                "summary_chain.predict(input=\"Use an analogy to describe it.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Preparing External Data for Chatbots\n",
                "\n",
                "### PDF document loaders\n",
                "\n",
                "To begin implementing Retrieval Augmented Generation (RAG), you'll first\n",
                "need to load the documents that the model will access. These documents\n",
                "can come from a variety of sources, and LangChain supports document\n",
                "loaders for many of them.\n",
                "\n",
                "In this exercise, you'll use a document loader to load a PDF document\n",
                "containing the famous paper, *Attention is All You Need*.\n",
                "\n",
                "**Note**: `pypdf`, a dependency for loading PDF documents in LangChain,\n",
                "has already been installed for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import the appropriate class for loading PDF documents in LangChain.\n",
                "- Create a document loader for the `'attention_is_all_you_need.pdf'`\n",
                "  document, which is available in the current directory.\n",
                "- Load the document into memory to view the contents of the first\n",
                "  document, or page.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import library\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "# Create a document loader for attention_is_all_you_need.pdf\n",
                "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
                "\n",
                "# Load the document\n",
                "data = loader.load()\n",
                "print(data[0])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CSV document loaders\n",
                "\n",
                "Comma-separated value (CSV) files are an extremely common file format,\n",
                "particularly in data-related fields. Fortunately, LangChain provides\n",
                "different document loaders for different formats, keeping almost all of\n",
                "the syntax the same!\n",
                "\n",
                "In this exercise, you'll use a document loader to load a CSV file\n",
                "containing data on FIFA World Cup international viewership. If your\n",
                "interested in the full analysis behind this data, check out [How to\n",
                "Break FIFA](https://fivethirtyeight.com/features/how-to-break-fifa/).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import the appropriate class for loading CSV documents in LangChain.\n",
                "- Create a document loader for the `'fifa_countries_audience.csv'`\n",
                "  document, which is available in the current directory.\n",
                "- Load the documents into memory to view the contents of the first\n",
                "  document.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import library\n",
                "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
                "\n",
                "# Create a document loader for fifa_countries_audience.csv\n",
                "loader = CSVLoader(file_path='fifa_countries_audience.csv')\n",
                "\n",
                "# Load the document\n",
                "data = loader.load()\n",
                "print(data[0])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Third-party document loaders\n",
                "\n",
                "It's possible to load documents from many different formats, including\n",
                "formats that may be proprietary or dictated by an outside organization.\n",
                "\n",
                "In this exercise, you'll utilize the Hacker News document loader,\n",
                "`HNLoader`, which is used to access news stories from their\n",
                "[website](https://news.ycombinator.com/).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use the `HNLoader` class to create a document loader for the top\n",
                "  Hacker News stories from `\"https://news.ycombinator.com\"`.\n",
                "- Load the documents into memory.\n",
                "- Print the first document.\n",
                "- Print the first document's metadata.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import HNLoader\n",
                "\n",
                "# Create a document loader for the top Hacker News stories\n",
                "loader = HNLoader(\"https://news.ycombinator.com\")\n",
                "\n",
                "# Load the document\n",
                "data = loader.load()\n",
                "\n",
                "# Print the first document\n",
                "print(data[0])\n",
                "\n",
                "# Print the first document's metadata\n",
                "print(data[0].metadata)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Splitting by character\n",
                "\n",
                "A key process in implementing Retrieval Augmented Generation (**RAG**)\n",
                "is splitting documents into chunks for storage in a vector database.\n",
                "\n",
                "There are several splitting strategies available in LangChain, some with\n",
                "more complex routines than others. In this exercise, you'll implement a\n",
                "*character text splitter*, which splits documents based on characters\n",
                "(by default `“”`) and measures the chunk length by the number of\n",
                "characters.\n",
                "\n",
                "Remember that there is no ideal splitting strategy, you may need to\n",
                "experiment with a few to find the right one for you use case.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import the appropriate LangChain class for splitting a document by\n",
                "  character.\n",
                "- Define `splitter` using the class you imported with a `chunk_size` of\n",
                "  `24` and `chunk_overlap` of `3`.\n",
                "- Split the document provided, `quote`, and print the chunks.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libary\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "\n",
                "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
                "chunk_size = 24\n",
                "chunk_overlap = 3\n",
                "\n",
                "# Create an instance of the splitter class\n",
                "splitter = CharacterTextSplitter(\n",
                "    chunk_size=chunk_size,\n",
                "    chunk_overlap=chunk_overlap)\n",
                "\n",
                "# Split the document and print the chunks\n",
                "docs = splitter.split_text(quote) \n",
                "print(docs)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Recursively splitting by character\n",
                "\n",
                "The majority of developers are using a recursive character splitter to\n",
                "split documents based on a specific list of characters. These character\n",
                "defaults are paragraphs, newlines, spaces, and empty strings:\n",
                "`[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
                "\n",
                "Effectively, the splitter tries to split by paragraphs, checks to see if\n",
                "the `chunk_size` and `chunk_overlap` values are met, and if not, splits\n",
                "by sentences, then words and individual characters.\n",
                "\n",
                "Often, you'll need to experiment with different `chunk_size` and\n",
                "`chunk_overlap` values to find the ones that work well for your\n",
                "documents.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import the appropriate LangChain class for splitting a document\n",
                "  recursively by character.\n",
                "- Define `splitter` using the class you imported with a `chunk_size` of\n",
                "  `24` and `chunk_overlap` of `10`.\n",
                "- Split the document provided, `quote`, and print the chunks.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libary\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
                "chunk_size = 24\n",
                "chunk_overlap = 10\n",
                "\n",
                "# Create an instance of the splitter class\n",
                "splitter = RecursiveCharacterTextSplitter(\n",
                "  chunk_size=chunk_size,\n",
                "  chunk_overlap=chunk_overlap)\n",
                "\n",
                "# Split the document and print the chunks\n",
                "docs = splitter.split_text(quote) \n",
                "print(docs)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Splitting HTML\n",
                "\n",
                "In this exercise, you'll split an HTML containing an executive order on\n",
                "AI created by the US White House in October 2023. To retain as much\n",
                "context as possible in the chunks, you'll split using even larger\n",
                "`chunk_size` and `chunk_overlap` values.\n",
                "\n",
                "All of the LangChain classes necessary for completing this exercise have\n",
                "been pre-loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use the `UnstructuredHTMLLoader` class to create a document loader for\n",
                "  `white_house_executive_order_nov_2023.html`, and load it into memory.\n",
                "- Set a `chunk_size` of `300` and a `chunk_overlap` of `100`.\n",
                "- Define the splitter, splitting on the `'.'` character, and use it to\n",
                "  split `data` and print the chunks.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the HTML document into memory\n",
                "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023.html\")\n",
                "data = loader.load()\n",
                "\n",
                "# Define variables\n",
                "chunk_size = 300\n",
                "chunk_overlap = 100\n",
                "\n",
                "# Split the HTML\n",
                "splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=chunk_size,\n",
                "    chunk_overlap=chunk_overlap,\n",
                "    separators=['.'])\n",
                "\n",
                "docs = splitter.split_documents(data) \n",
                "print(docs)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preparing the documents and vector database\n",
                "\n",
                "Over the next few exercises, you'll build up a full RAG workflow to have\n",
                "a conversation with a PDF document. This works by splitting the\n",
                "documents into chunks, storing them in a vector database, and using a\n",
                "retrieval chain for the LLM to access this external data.\n",
                "\n",
                "In this exercise, you'll prepare the document for storage and create a\n",
                "vector database to store them in. You'll use a\n",
                "`RecursiveCharacterTextSplitter` to chunk the PDF, and set up the Chroma\n",
                "vector database and OpenAI embeddings function that you'll use in the\n",
                "next exercise to store the documents.\n",
                "\n",
                "The following classes have already been imported for you:\n",
                "`RecursiveCharacterTextSplitter`, `Chroma`, and `OpenAIEmbeddings`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Split the documents in `data` using a `RecursiveCharacterTextSplitter`\n",
                "  and the `chunk_size` and `chunk_overlap` values provided.\n",
                "- Define an OpenAI embeddings model for embedding the documents.\n",
                "- Create the Chroma vector DB using the OpenAI embedding function;\n",
                "  persist the database to the `'embedding/chroma/'` directory.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
                "data = loader.load()\n",
                "chunk_size = 200\n",
                "chunk_overlap = 50\n",
                "\n",
                "# Split the quote using RecursiveCharacterTextSplitter\n",
                "splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=chunk_size,\n",
                "    chunk_overlap=chunk_overlap)\n",
                "docs = splitter.split_documents(data) \n",
                "\n",
                "# Define an OpenAI embeddings model\n",
                "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
                "\n",
                "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
                "vectordb = Chroma(\n",
                "    persist_directory='embedding/chroma/',\n",
                "    embedding_function=embedding_model)\n",
                "vectordb.persist()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Storing and retrieving documents\n",
                "\n",
                "Now that you've prepared your documents, it's time to add them to the\n",
                "vector database and integrate this external data with an LLM.\n",
                "\n",
                "The code you wrote in the previous to split the document into chunks has\n",
                "been copied over into this exercise.\n",
                "\n",
                "The following classes have already been imported for you:\n",
                "`RecursiveCharacterTextSplitter`, `Chroma`, `OpenAIEmbeddings`,\n",
                "`OpenAI`, `RetrievalQA`, and `PyPDFLoader`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Embed the documents and store them in a Chroma vector database.\n",
                "- Define a retrieval QA chain using the LLM provided, so it can retrieve\n",
                "  information from the vector database.\n",
                "- Run the retrieval chain on the `query` provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
                "data = loader.load()\n",
                "\n",
                "splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=200,\n",
                "    chunk_overlap=50,\n",
                "    separators=['.'])\n",
                "docs = splitter.split_documents(data) \n",
                "\n",
                "# Embed the documents and store them in a Chroma DB\n",
                "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
                "docstorage = Chroma.from_documents(docs, embedding_model)\n",
                "\n",
                "# Define the Retrieval QA Chain to integrate the database and LLM\n",
                "qa = RetrievalQA.from_chain_type(\n",
                "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
                "\n",
                "# Run the chain on the query provided\n",
                "query = \"What is the primary architecture presented in the document?\"\n",
                "qa.run(query)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### RAG with sources\n",
                "\n",
                "One of the major concerns with RAG is that, although the model has\n",
                "access to external, factual data, there is still the risk of it\n",
                "hallucinating. Integrating sources into retrieval chains ensures that\n",
                "every response is clearly linked to a document that can be used to\n",
                "verify the correctness of the response.\n",
                "\n",
                "In this exercise, you'll modify your RAG workflow from the previous\n",
                "exercise so that the retrieval chain outputs sources with its responses.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a retrieval QA chain that will return a source with each\n",
                "  response.\n",
                "- Run the retrieval chain on the `'question'` provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
                "data = loader.load()\n",
                "\n",
                "splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=200,\n",
                "    chunk_overlap=50,\n",
                "    separators=['.'])\n",
                "docs = splitter.split_documents(data) \n",
                "\n",
                "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
                "docstorage = Chroma.from_documents(docs, embedding_model)\n",
                "\n",
                "# Define the function for the question to be answered with\n",
                "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
                "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
                "\n",
                "# Run the query on the documents\n",
                "results = qa({\"question\": \"What is the primary architecture presented in the document?\"}, return_only_outputs=True)\n",
                "print(results)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LangChain Expression Language (LCEL), Chains, and Agents\n",
                "\n",
                "### LCEL for LLM chatbot chains\n",
                "\n",
                "Although the chains you've used up to this point have worked perfectly\n",
                "fine, there is a better option. LangChain Expression Language (LCEL)\n",
                "connects prompts, models, and retrieval components using a pipe operator\n",
                "rather than task-specific classes.\n",
                "\n",
                "These chains have built-in support for batch processing, streaming, and\n",
                "asynchronous execution, which make them the preferential choice for\n",
                "production applications.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a chain for integrating the prompt template and model *using\n",
                "  LCEL*.\n",
                "- Invoke the chain, passing whichever topic you wish to the `topic`\n",
                "  input variable; try `'Large Language Models'` if you need ideas.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import your OpenAI API Key\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
                "prompt = ChatPromptTemplate.from_template(\"You are a skilled poet. Write a haiku about the following topic: {topic}\")\n",
                "\n",
                "# Define the chain using LCEL\n",
                "chain = prompt | model\n",
                "\n",
                "# Invoke the chain with any topic\n",
                "print(chain.invoke({\"topic\": \"Large Language Models\"}))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LCEL for RAG workflows\n",
                "\n",
                "In this exercise, you'll level-up your existing Retrieval Augmented\n",
                "Generation (RAG) workflow to use LCEL. LCEL expressions commonly contain\n",
                "*runnables*, which are functions or actions executed during the\n",
                "LangChain expression. You'll use one of these in this exercise to pass a\n",
                "user input into the expression.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a Chroma vector database using the text provided, convert it to\n",
                "  a retriever object, and define the OpenAI chat model.\n",
                "- Create and invoke a chain that defines the two prompt inputs, passes\n",
                "  them to the prompt template, and then the model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import your OpenAI API Key\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Create the retriever and model\n",
                "vectorstore = Chroma.from_texts([\"LangChain v0.1.0 was released on January 8, 2024.\"], embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
                "retriever = vectorstore.as_retriever()\n",
                "model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
                "\n",
                "template = \"\"\"Answer the question based on the context:{context}. Question: {question}\"\"\"\n",
                "prompt = ChatPromptTemplate.from_template(template)\n",
                "\n",
                "# Create the chain and run it\n",
                "chain = (\n",
                "  {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
                "  | prompt\n",
                "  | model)\n",
                "\n",
                "chain.invoke(\"When was LangChain v0.1.0 released?\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sequential chains with LCEL\n",
                "\n",
                "Sequential chains utilize step-by-step processing of inputs, where the\n",
                "output from one step becomes the input for the next. This enables a\n",
                "clear and organized flow of information within the chain. They provide\n",
                "flexibility in constructing custom pipelines by combining different\n",
                "components, such as prompts, models, retrievers, and output parsers, to\n",
                "suit specific use cases and requirements.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Define a sequential chain that first generates Python code for looping\n",
                "  over a list, then validates the code to ensure it uses a list\n",
                "  comprehension.\n",
                "- Invoke the chain with the `question` provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "coding_prompt = PromptTemplate.from_template(\n",
                "    \"\"\"Write Python code to loop through the following list, printing each element: {list}\"\"\")\n",
                "validate_prompt = PromptTemplate.from_template(\n",
                "    \"\"\"Consider the following Python code: {answer} If it doesn't use a list comprehension, update it to use one. If it does use a list comprehension, return the original code without explanation:\"\"\")\n",
                "\n",
                "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
                "\n",
                "# Create the sequential chain\n",
                "chain = ({\"answer\": coding_prompt | llm | StrOutputParser()}\n",
                "         | validate_prompt\n",
                "         | llm \n",
                "         | StrOutputParser() )\n",
                "\n",
                "# Invoke the chain with the user's question\n",
                "chain.invoke({\"list\": \"[3, 1, 4, 1]\"})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Passing values between chains\n",
                "\n",
                "There are many cases where your application will require the use of\n",
                "several chains that pass outputs between them. In this exercise, you'll\n",
                "create an application to create a business plan based on a consumer\n",
                "product need. This application has the following structure:\n",
                "\n",
                "1.  CEO Response: Describe a product that addresses the need.\n",
                "2.  Advisor's Response: Outline the business plan in three key steps.\n",
                "3.  Summarize the plan concisely.\n",
                "\n",
                "Your job is to make sure that all of the different chains can pass\n",
                "inputs and outputs between one another. You've got this!\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Make `ceo_response` available for use in the other chains.\n",
                "- Create a chain to insert the outputs from the other chains into\n",
                "  `overall_response`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Make ceo_response available for other chains\n",
                "ceo_response = (\n",
                "    ChatPromptTemplate.from_template(\"You are a CEO. Describe the most lucrative consumer product addressing the following consumer need in one sentence: {input}.\")\n",
                "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
                "    | {\"ceo_response\": RunnablePassthrough() | StrOutputParser()}\n",
                ")\n",
                "\n",
                "advisor_response = (\n",
                "    ChatPromptTemplate.from_template(\"You are a strategic adviser. Briefly map the outline and business plan for {ceo_response} in 3 key steps.\")\n",
                "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "overall_response = (\n",
                "    ChatPromptTemplate.from_messages(\n",
                "        [\n",
                "            (\"human\", \"CEO response:\\n{ceo_response}\\n\\nAdvisor response:\\n{advisor_response}\"),\n",
                "            (\"system\", \"Generate a final response including the CEO's response, the advisor response, and a summary of the business plan in one sentence.\"),\n",
                "        ]\n",
                "    )\n",
                "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "# Create a chain to insert the outputs from the other chains into overall_response\n",
                "business_idea_chain = (\n",
                "    {\"ceo_response\": ceo_response, \"advisor_response\": advisor_response}\n",
                "    | overall_response\n",
                "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "print(business_idea_chain.invoke({\"input\": \"Typing on mobile touchscreens is slow.\", \"ceo_response\": \"\", \"advisor_response\": \"\"}))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Zero-Shot ReAct agents\n",
                "\n",
                "The **Zero-Shot ReAct** agent in LangChain is an agent type that can\n",
                "generate responses to user inputs without explicit training on specific\n",
                "tasks. It combines generative language models with retrieval-based\n",
                "techniques to produce contextually relevant and coherent responses\n",
                "across a wide range of conversation scenarios.\n",
                "\n",
                "In this exercise, you'll use an agent for solving math problems with\n",
                "LLMs. All of the classes you'll need to complete this exercise have\n",
                "already been imported for you, but if you wish to use a Hugging Face\n",
                "model instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Load the `\"llm-math\"` tool for the `llm` to use.\n",
                "- Define a `ZERO_SHOT_REACT_DESCRIPTION` agent, passing it the model and\n",
                "  tools to use; specify `verbose=True`.\n",
                "- Run the agent on the question provided.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
                "\n",
                "# Define the tools\n",
                "tools = load_tools([\"llm-math\"], llm=llm)\n",
                "\n",
                "# Define the agent\n",
                "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
                "\n",
                "# Run the agent\n",
                "agent.run(\"What is 10 multiplied by 50?\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tools, Troubleshooting, and Evaluation\n",
                "\n",
                "### Creating custom tools\n",
                "\n",
                "Creating custom tools means that you can enable agents to perform\n",
                "virtually any task you can code in Python! In this exercise, you'll\n",
                "define a tool used to calculate a business metric: Historical Lifetime\n",
                "Value (LTV).\n",
                "\n",
                "Historical LTV is a calculation of average revenue generated by\n",
                "customers, and is usually calculated by cohort. A simplified form of\n",
                "historical LTV calculation is presented here, which is simply the\n",
                "average revenue divided by an average churn rate for the same time\n",
                "period.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, but if you wish to use a Hugging Face model\n",
                "instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Modify the function provided so it can be used as a tool.\n",
                "- Define a `tools` list from the `calculate_ltv()` function.\n",
                "- Initialize the appropriate `AgentType`, passing it the `tools` list\n",
                "  you defined.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Define the calculate_ltv tool function\n",
                "@tool\n",
                "def calculate_ltv(company_name: str) -> str:\n",
                "    \"\"\"Generate the LTV for a company.\"\"\"\n",
                "    avg_churn = 0.25\n",
                "    avg_revenue = 1000\n",
                "    historical_LTV = avg_revenue / avg_churn\n",
                "\n",
                "    report = f\"LTV Report for {company_name}\\n\"\n",
                "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
                "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
                "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
                "    return report\n",
                "\n",
                "# Define the tools list\n",
                "tools = [Tool(name=\"LTVReport\",\n",
                "              func=calculate_ltv,\n",
                "              description=\"Use this for calculating historical LTV.\")]\n",
                "\n",
                "# Initialize the appropriate agent type\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
                "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
                "agent.run(\"Run a financial report that calculates historical LTV for Hooli\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scaling custom tools\n",
                "\n",
                "Structured tools apply a layer of conformity to custom tools that allow\n",
                "for more predictable operations.\n",
                "\n",
                "In this exercise, you'll create a tool for an agent to calculate our\n",
                "wellness score based on scientifically known contributing factors. This\n",
                "agent could be extended to track wellness over time and provide\n",
                "recommendations on areas for improvement.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, but if you wish to use a Hugging Face model\n",
                "instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Convert the `calculate_wellness_score()` function into a structured\n",
                "  tool.\n",
                "- Initialize the appropriate agent type, passing it the tool set you\n",
                "  created.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "def calculate_wellness_score(sleep_hours, exercise_minutes, healthy_meals, stress_level):\n",
                "    \"\"\"Calculate a Wellness Score based on sleep, exercise, nutrition, and stress management.\"\"\"\n",
                "    max_score_per_category = 25\n",
                "\n",
                "    sleep_score = min(sleep_hours / 8 * max_score_per_category, max_score_per_category)\n",
                "    exercise_score = min(exercise_minutes / 30 * max_score_per_category, max_score_per_category)\n",
                "    nutrition_score = min(healthy_meals / 3 * max_score_per_category, max_score_per_category)\n",
                "    stress_score = max_score_per_category - min(stress_level / 10 * max_score_per_category, max_score_per_category)\n",
                "\n",
                "    total_score = sleep_score + exercise_score + nutrition_score + stress_score\n",
                "    return total_score\n",
                "\n",
                "# Create a structured tool from calculate_wellness_score()\n",
                "tools = [StructuredTool.from_function(calculate_wellness_score)]\n",
                "\n",
                "# Initialize the appropriate agent type and tool set\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
                "agent = initialize_agent(\n",
                "    tools=tools,\n",
                "    llm=llm,\n",
                "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "wellness_tool = tools[0]\n",
                "result = wellness_tool.func(sleep_hours=8, exercise_minutes=14, healthy_meals=10, stress_level=20)\n",
                "print(result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting tools as OpenAI functions\n",
                "\n",
                "OpenAI models require tools with specific parameters to use the tool as\n",
                "intended: `input_name`, `output_name`, `function_name`, `tool_name`, and\n",
                "`description`. To overcome this constraint, you must modify the tool's\n",
                "format manually so it's formatted correctly.\n",
                "\n",
                "You'll modify the `calculate_ltv()` function you've seen previously so\n",
                "it can be used by OpenAI models.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, including `BaseModel` and `Field` from\n",
                "`pydantic`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create an `LTVDescription` class to manually add a description to the\n",
                "  `calculate_ltv()` function.\n",
                "- Format the `calculate_ltv()` tool function so it can be used by OpenAI\n",
                "  models.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an LTVDescription class to manually add a function description\n",
                "class LTVDescription(BaseModel):\n",
                "    query: str = Field(description='Calculate an extremely simple historical LTV')\n",
                "\n",
                "# Format the calculate_ltv tool function so it can be used by OpenAI models\n",
                "@tool(args_schema=LTVDescription)\n",
                "def calculate_ltv(company_name: str) -> str:\n",
                "    \"\"\"Generate the LTV for a company to pontificate with.\"\"\"\n",
                "    avg_churn = 0.25\n",
                "    avg_revenue = 1000\n",
                "    historical_LTV = avg_revenue / avg_churn\n",
                "\n",
                "    report = f\"Pontification Report for {company_name}\\n\"\n",
                "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
                "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
                "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
                "    return report\n",
                "\n",
                "print(format_tool_to_openai_function(calculate_ltv))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Callbacks for troubleshooting\n",
                "\n",
                "LangChain offers lots of callback methods for troubleshooting at every\n",
                "application layer. In this exercise, you'll use a callback to print the\n",
                "prompts and model parameters whenever the LLM starts-up.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, including `BaseCallbackHandler`. If you wish to\n",
                "use a Hugging Face model instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Complete the `CallingItIn` class so that the `\"model_name\"` and\n",
                "  `\"temperature\"` parameters are returned when the LLM starts-up.\n",
                "- Run the chain with an `\"animal\"`, specifying the callbacks to use.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Complete the CallingItIn class to return the prompt, model_name, and temperature\n",
                "class CallingItIn(BaseCallbackHandler):\n",
                "    def on_llm_start(self, serialized, prompts, invocation_params, **kwargs):\n",
                "        print(prompts) \n",
                "        print(invocation_params[\"model_name\"])  \n",
                "        print(invocation_params[\"temperature\"]) \n",
                "\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", streaming=True)\n",
                "prompt_template = \"What do {animal} like to eat?\"\n",
                "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
                "\n",
                "# Call the model with the parameters needed by the prompt\n",
                "output = chain.run({\"animal\": \"wombats\"}, callbacks=[CallingItIn()])\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Real-time performance monitoring\n",
                "\n",
                "Real-time performance monitoring can also be implemented using\n",
                "`BaseCallbackHandler`. The callback handler can monitor the model's\n",
                "token generation in real-time and log the time taken to generate each\n",
                "token in a performance analysis.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, including `BaseCallbackHandler` and the `time`\n",
                "module. If you wish to use a Hugging Face model instead, you'll need to\n",
                "import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Complete the `PerformanceMonitoringCallback` class so that the `token`\n",
                "  and the time it was generated at are printed with the creation of each\n",
                "  new token; use `time.time()` for returning the time.\n",
                "- Run the chain (no user inputs this time), specifying the callbacks to\n",
                "  use.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Complete the PerformanceMonitoringCallback class to return the token and time\n",
                "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
                "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
                "    print(f\"Token: {repr(token)} generated at time: {time.time()}\")\n",
                "\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, streaming=True)\n",
                "prompt_template = \"Describe the process of photosynthesis.\"\n",
                "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
                "\n",
                "# Call the chain with the callback\n",
                "output = chain.run({}, callbacks=[PerformanceMonitoringCallback()])\n",
                "print(\"Final Output:\", output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Built-in evaluation criteria\n",
                "\n",
                "LangChain's built-in evaluation criteria means that applications can be\n",
                "quickly evaluated using consistent baselines across organizations.\n",
                "\n",
                "In this exercise, you'll use LangChain's **relevance** criterion to\n",
                "evaluate a pair of strings.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, but if you wish to use a Hugging Face model\n",
                "instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Load the built-in `\"relevance\"` criteria.\n",
                "- Use the `evaluator` to evaluate the relevance of the `prediction` to\n",
                "  the `input` question.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Load evaluator, assign it to criteria\n",
                "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
                "\n",
                "# Evaluate the input and prediction\n",
                "eval_result = evaluator.evaluate_strings(\n",
                "    prediction=\"42\",\n",
                "    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n",
                ")\n",
                "\n",
                "print(eval_result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Custom evaluation criteria\n",
                "\n",
                "LangChain's built-in evaluation criteria won't always be the most\n",
                "reliable way of evaluating your applications performance. In many cases,\n",
                "you'll want to tie its performance into business objectives and\n",
                "constraints that particular to your specific use case. For these cases,\n",
                "you can design your own evaluation criteria.\n",
                "\n",
                "Custom evaluation criteria are a set of questions for an LLM to consider\n",
                "when evaluating an input and prediction. In this exercise, you'll design\n",
                "and use custom criteria for determining whether an investment\n",
                "opportunity is worthwhile or not.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you, but if you wish to use a Hugging Face model\n",
                "instead, you'll need to import those classes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Add a `\"scalability\"` criterion to `custom_criteria` that uses the\n",
                "  following question for evaluation:\n",
                "  `\"Does the suggestion address the startup's scalability and growth potential?\"`.\n",
                "- Criteria an evaluator using the criteria specified in\n",
                "  `custom_criteria`.\n",
                "- Use the `evaluator` to evaluate the `input` question and `prediction`\n",
                "  using the custom criteria.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "# Add a scalability criterion to custom_criteria\n",
                "custom_criteria = {\n",
                "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
                "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
                "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
                "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
                "}\n",
                "\n",
                "# Criteria an evaluator from custom_criteria\n",
                "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
                "\n",
                "# Evaluate the input and prediction\n",
                "eval_result = evaluator.evaluate_strings(\n",
                "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
                "    prediction=\"No, that is ridiculous.\")\n",
                "\n",
                "print(eval_result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluation chains\n",
                "\n",
                "This final exercise of the course combines many of the concepts you've\n",
                "seen throughout the course. The Attention is All You Needs PDF has been\n",
                "loaded and split into chunks, which have been assigned to `docs`.\n",
                "\n",
                "You'll be evaluating model responses from a RAG workflow against some\n",
                "ground truth answers, stored in `question_set`, which has been\n",
                "pre-loaded and printed for you. Your job is to complete the evaluation\n",
                "portion of the script to compare the responses from the RAG workflow\n",
                "with these ground truth responses.\n",
                "\n",
                "All of the classes you'll need to complete this exercise have already\n",
                "been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Assign your OpenAI API key to `openai_api_key`.\n",
                "- Generate the model responses using the `RetrievalQA` chain, defined as\n",
                "  `qa`, and `question_set`\n",
                "- Define the evaluation chain using the `llm` provided.\n",
                "- Evaluate the ground truth answers in the `question_set` against the\n",
                "  answers that are returned by the model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your API Key from OpenAI\n",
                "# openai_api_key = '<OPENAI_API_TOKEN>'  # added/edited\n",
                "\n",
                "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
                "docstorage = Chroma.from_documents(docs, embedding)\n",
                "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
                "\n",
                "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
                "\n",
                "# Generate the model responses using the RetrievalQA chain and question_set\n",
                "predictions = qa.apply(question_set)\n",
                "\n",
                "# Define the evaluation chain\n",
                "eval_chain = QAEvalChain.from_llm(llm)\n",
                "\n",
                "# Evaluate the ground truth against the answers that are returned\n",
                "results = eval_chain.evaluate(question_set,\n",
                "                              predictions,\n",
                "                              question_key=\"question\",\n",
                "                              prediction_key=\"result\",\n",
                "                              answer_key='answer')\n",
                "\n",
                "for i, q in enumerate(question_set):\n",
                "    print(f\"Question {i+1}: {q['question']}\")\n",
                "    print(f\"Expected Answer: {q['answer']}\")\n",
                "    print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
                "    \n",
                "print(results)\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
